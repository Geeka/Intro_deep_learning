{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "7Cj6KbLpKoOd"
            },
            "source": "<a href=\"http://cocl.us/pytorch_link_top\">\n    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n</a> "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "fIiVRCmYKoOi"
            },
            "source": "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "9v60yw28KoOj"
            },
            "source": "<h1><h1>Pre-trained-Models with PyTorch </h1>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "hLXXiTH0KoOk"
            },
            "source": "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n<ul>\n<li>change the output layer</li>\n<li> train the model</li> \n<li>  identify  several  misclassified samples</li> \n </ul>\nYou will take several screenshots of your work and share your notebook. "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "QByHCjABKoOm"
            },
            "source": "<h2>Table of Contents</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "B9dp6pfwKoOo"
            },
            "source": "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n\n<ul>\n    <li><a href=\"#download_data\"> Download Data</a></li>\n    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n    <li><a href=\"#data_class\"> Dataset Class</a></li>\n    <li><a href=\"#Question_1\">Question 1</a></li>\n    <li><a href=\"#Question_2\">Question 2</a></li>\n    <li><a href=\"#Question_3\">Question 3</a></li>\n</ul>\n<p>Estimated Time Needed: <strong>120 min</strong></p>\n </div>\n<hr>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "7trVVuPaKoOp"
            },
            "source": "<h2 id=\"download_data\">Download Data</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "_xOTpTyTKoOq"
            },
            "source": "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 204
                },
                "colab_type": "code",
                "id": "OqaIMeQSKoOt",
                "outputId": "90a2adc3-e84d-4948-a442-9a8cfd014dc4",
                "scrolled": false
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-03-03 02:14:01--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2598656062 (2.4G) [application/zip]\nSaving to: \u2018Positive_tensors.zip\u2019\n\nPositive_tensors.zi 100%[===================>]   2.42G  37.9MB/s    in 65s     \n\n2020-03-03 02:15:06 (38.4 MB/s) - \u2018Positive_tensors.zip\u2019 saved [2598656062/2598656062]\n\n"
                }
            ],
            "source": "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "colab_type": "code",
                "id": "vEkTNiXMLQ82",
                "outputId": "810dbf81-8314-4d2a-8055-8c53156cd2e9"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "'/content'"
                    },
                    "execution_count": 2,
                    "metadata": {
                        "tags": []
                    },
                    "output_type": "execute_result"
                }
            ],
            "source": "%pwd"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "2U1yAETDKoOz",
                "scrolled": true
            },
            "outputs": [],
            "source": "!unzip -q Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 204
                },
                "colab_type": "code",
                "id": "8msXp38kKoO3",
                "outputId": "56141638-bbf1-416e-ebf1-334681ebd258",
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-03-03 02:20:15--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2111408108 (2.0G) [application/zip]\nSaving to: \u2018Negative_tensors.zip\u2019\n\nNegative_tensors.zi 100%[===================>]   1.97G  44.9MB/s    in 51s     \n\n2020-03-03 02:21:06 (39.8 MB/s) - \u2018Negative_tensors.zip\u2019 saved [2111408108/2111408108]\n\n"
                }
            ],
            "source": "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n!unzip -q Negative_tensors.zip"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "tWDMo8LIKoO7"
            },
            "source": "We will install torchvision:"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 102
                },
                "colab_type": "code",
                "id": "WQVKYMYUKoO8",
                "outputId": "44a91b68-92a1-4bda-b875-3440f1b0e2d3"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\nRequirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.5)\nRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.2)\n"
                }
            ],
            "source": "!pip install torchvision"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "J8SfG4A2KoPB"
            },
            "source": "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "UWiX2DhMKoPC"
            },
            "source": "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it."
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "colab_type": "code",
                "id": "_PVcbjgsKoPD",
                "outputId": "5d92f9a0-5dda-42f7-a1ce-086586e69772"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "<torch._C.Generator at 0x7ff41baea430>"
                    },
                    "execution_count": 6,
                    "metadata": {
                        "tags": []
                    },
                    "output_type": "execute_result"
                }
            ],
            "source": "# These are the libraries will be used for this lab.\nimport torchvision.models as models\nfrom PIL import Image\nimport pandas\nfrom torchvision import transforms\nimport torch.nn as nn\nimport time\nimport torch \nimport matplotlib.pylab as plt\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport h5py\nimport os\nimport glob\ntorch.manual_seed(0)"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "e1lsvySwKoPI"
            },
            "outputs": [],
            "source": "from matplotlib.pyplot import imshow\nimport matplotlib.pylab as plt\nfrom PIL import Image\nimport pandas as pd\nimport os"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "jvmHmTZLKoPL"
            },
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "cHiTyt5oKoPM"
            },
            "source": "<h2 id=\"data_class\">Dataset Class</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "ddYI_JiHKoPN"
            },
            "source": " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step."
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "colab_type": "code",
                "id": "jgDNfHb-KoPO",
                "outputId": "6e102588-628b-4bf0-d0b6-f7320bbfcd72"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "# Create your own dataset object\n\nclass Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        directory=\"/content\"\n        positive=\"Positive_tensors\"\n        negative='Negative_tensors'\n\n        positive_file_path=os.path.join(directory,positive)\n        negative_file_path=os.path.join(directory,negative)\n        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n        number_of_samples=len(positive_files)+len(negative_files)\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n            self.all_files=self.all_files[0:30000]\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n            self.all_files=self.all_files[30000:]\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)     \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n               \n        image=torch.load(self.all_files[idx])\n        y=self.Y[idx]\n                  \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y\n    \nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "cGFSU4bZKoPS"
            },
            "source": "We create two dataset objects, one for the training data and one for the validation data."
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "colab_type": "code",
                "id": "K47DFB5AKoPT",
                "outputId": "8859f7fb-98ec-42a8-ab93-cc7fb4886300"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "train_dataset = Dataset(train=True)\nvalidation_dataset = Dataset(train=False)\nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "iJgb0DyaKoPW"
            },
            "source": "<h2 id=\"Question_1\">Question 1</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "3m5norUXKoPX"
            },
            "source": "<b>Prepare a pre-trained resnet18 model :</b>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "6FpurGh9KoPY"
            },
            "source": "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "M1QCE0QJOCtQ"
            },
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 83,
                    "referenced_widgets": [
                        "d0e8c4784d8b471fbd0a1550825470d2",
                        "6e92ae70a84142bbb875434128664da4",
                        "b29c4de7c4cb4f6abdde19dbe4c8ae1a",
                        "921d678d2189460aa0eef94d863927b0",
                        "d516a728cc0c4ce59bfffc0c784c77c4",
                        "20a2728175084e69b508cab77d8195f7",
                        "4ff794d274f3412fbc373d186dc3a430",
                        "f1f0b027576d4dc1a1849a37e84405a4"
                    ]
                },
                "colab_type": "code",
                "id": "bk5NfauJKoPZ",
                "outputId": "052062bc-25d8-4229-9a02-587359b426f2"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d0e8c4784d8b471fbd0a1550825470d2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "HBox(children=(IntProgress(value=0, max=46827520), HTML(value='')))"
                    },
                    "metadata": {
                        "tags": []
                    },
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\n"
                }
            ],
            "source": "# Step 1: Load the pre-trained model resnet18\n\n# Type your code here\nimport torchvision.models as models\nmodel =models.resnet18(pretrained=True)"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "l3648lYcODTu"
            },
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "7mMU6lYwKoPc"
            },
            "source": "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training."
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "colab_type": "code",
                "id": "pCvgJE-HKoPf",
                "outputId": "976fb2a3-a1d1-4341-f4f4-36357e1555e8"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)"
                    },
                    "execution_count": 12,
                    "metadata": {
                        "tags": []
                    },
                    "output_type": "execute_result"
                }
            ],
            "source": "# Step 2: Set the parameter cannot be trained for the pre-trained model\nfor param in model.parameters():\n    param.requires_grad=False\n\n# Type your code here\nmodel"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "dG97riY5KoPj"
            },
            "source": "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "O55zSPCZKoPk"
            },
            "source": "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons."
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "PMl24q7gKoPl"
            },
            "outputs": [],
            "source": "model.fc=nn.Linear(512,2)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "YSXecdrDKoPp"
            },
            "source": "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "colab_type": "code",
                "id": "RG_vEIyMKoPq",
                "outputId": "6d15fe5b-1428-45c8-c44b-6f6bc4c598e7",
                "scrolled": false
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n"
                }
            ],
            "source": "print(model)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "45P_Z7ORKoPu"
            },
            "source": "<h2 id=\"Question_2\">Question 2: Train the Model</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "fuNmXSKZKoPu"
            },
            "source": "In this question you will train your, model:"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "819CX6_RKoPv"
            },
            "source": "<b>Step 1</b>: Create a cross entropy criterion function "
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "t3SVkbXFKoPx"
            },
            "outputs": [],
            "source": "# Step 1: Create the loss function\n\n# Type your code here\ncriterion=nn.CrossEntropyLoss()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "C7HSO8kFKoP0"
            },
            "source": "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each."
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "_MG765sKKoP1"
            },
            "outputs": [],
            "source": "import torch.utils.data\ntrain_loader=DataLoader(dataset=train_dataset,batch_size=100)\nvalidation_loader=DataLoader(dataset=validation_dataset,batch_size=100)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "XoyunJR9KoP5"
            },
            "source": "<b>Step 3</b>: Use the following optimizer to minimize the loss "
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "ky37Wz3AKoP5"
            },
            "outputs": [],
            "source": "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "lazHFlinKoP8"
            },
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "P-mXgqD_KoP9"
            },
            "source": "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "GlqrZT_FKoP-"
            },
            "outputs": [],
            "source": "n_epochs=1\nloss_list=[]\naccuracy_list=[]\nmis_samples=[]\ncorrect=0\nN_test=len(validation_dataset)\nN_train=len(train_dataset)\nstart_time = time.time()\n#n_epochs\n"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 190
                },
                "colab_type": "code",
                "id": "AaMWQsvpPVqn",
                "outputId": "3ed5de6b-5bb8-4cd2-9aaa-42f64f47b022"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "training...\n\n1583203016.6885726\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, \n\n101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, \n\n201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, \n\n"
                }
            ],
            "source": "Loss=0\ni=0\nstart_time = time.time()\n\ni=0\nprint('training...\\n')\nprint(str(time.time()))\nfor x, y in train_loader:\n    \n    model.train() \n    i+=1\n    print(i,end=', ')\n    if(i%100==0):\n      print('\\n')\n    #clear gradient \n    optimizer.zero_grad()\n  \n    #make a prediction \n    z=model(x)\n\n    # calculate loss \n    loss=criterion(z,y)\n\n    # calculate gradients of parameters \n    loss.backward()\n    \n    # update parameters \n    optimizer.step()\n    \n    loss_list.append(loss.data)\n    \n    \n"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 122
                },
                "colab_type": "code",
                "id": "5n0Ppj1WKoQB",
                "outputId": "ab278952-77a5-4092-bfc9-3fba6e692981"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nEvaluating...\n\n1583205733.7658727\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, "
                }
            ],
            "source": "correct=0\ni1=0\n\nprint('\\nEvaluating...\\n')\nprint(str(time.time()))\nfor x_test, y_test in validation_loader:\n    # set model to eval \n    model.eval()\n    #make a prediction \n    z=model(x_test)\n    \n    #find max \n    _,yhat=torch.max(z.data,1)\n    i1+=1\n    \n    #Calculate misclassified  samples in mini-batch \n    #hint +=(yhat==y_test).sum().item()\n    correct+=(yhat==y_test).sum().item()\n    mis_samples.append()\n    print (i1,end=', ')        \n\naccuracy=correct/N_test\n\naccuracy_list.append(accuracy)"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "19BlYWJKPP9g"
            },
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "YjiUKu40KoQE"
            },
            "source": "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "colab_type": "code",
                "id": "kirzX4AfKoQF",
                "outputId": "50318cd5-e99f-46dd-fb3c-031e7653e4d0"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.9936"
                    },
                    "execution_count": 21,
                    "metadata": {
                        "tags": []
                    },
                    "output_type": "execute_result"
                }
            ],
            "source": "accuracy"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 279
                },
                "colab_type": "code",
                "id": "aYbQCQ_qKoQJ",
                "outputId": "e64b05ac-227e-4b35-9819-1d04033b8bcb"
            },
            "outputs": [
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "tags": []
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "plt.plot(loss_list)\nplt.xlabel(\"iteration\")\nplt.ylabel(\"loss\")\nplt.show()\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "f8LvTAF6KoQM"
            },
            "source": "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "pplHwK_KKoQM"
            },
            "source": "<b>Identify the first four misclassified samples using the validation data:</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 85
                },
                "colab_type": "code",
                "id": "MOo7P7HsKoQN",
                "outputId": "47aa0501-4362-4d29-a5d3-98a99d963548"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Sample 243 predicted value:tensor([1]) actual value:tensor([0])\nSample 608 predicted value:tensor([0]) actual value:tensor([1])\nSample 830 predicted value:tensor([0]) actual value:tensor([1])\nSample 844 predicted value:tensor([0]) actual value:tensor([1])\n"
                }
            ],
            "source": "i2=0\ncount=0\nvalidation_loader_1=DataLoader(dataset=validation_dataset,batch_size=1)\nfor x_test, y_test in validation_loader_1:\n    # set model to eval \n    model.eval()\n    #make a prediction \n    z=model(x_test)\n\n    #find max \n    _,yhat=torch.max(z.data,1)\n\n    #Calculate misclassified  samples in mini-batch \n    #hint +=(yhat==y_test).sum().item()\n    correct+=(yhat==y_test).sum().item()\n    if(yhat!=y_test):\n        print('Sample {} predicted value:{} actual value:{}'.format(i2,yhat,y_test))\n        count+=1\n    i2+=1\n    if(count>=4):\n        break\n    \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "LZQsEOmfKoQQ"
            },
            "source": "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html\"> CLICK HERE </a> Click here to see how to share your notebook."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "5cPTIjajKoQR"
            },
            "source": "<h2>About the Authors:</h2> \n\n<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "1HTLZWvFKoQS"
            },
            "source": "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "name": "Copy of AICapstoneFinal.ipynb",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        },
        "widgets": {
            "application/vnd.jupyter.widget-state+json": {
                "20a2728175084e69b508cab77d8195f7": {
                    "model_module": "@jupyter-widgets/base",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "4ff794d274f3412fbc373d186dc3a430": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "6e92ae70a84142bbb875434128664da4": {
                    "model_module": "@jupyter-widgets/base",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "921d678d2189460aa0eef94d863927b0": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_f1f0b027576d4dc1a1849a37e84405a4",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_4ff794d274f3412fbc373d186dc3a430",
                        "value": "100% 44.7M/44.7M [00:00&lt;00:00, 51.7MB/s]"
                    }
                },
                "b29c4de7c4cb4f6abdde19dbe4c8ae1a": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_name": "IntProgressModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "IntProgressModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "ProgressView",
                        "bar_style": "success",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_20a2728175084e69b508cab77d8195f7",
                        "max": 46827520,
                        "min": 0,
                        "orientation": "horizontal",
                        "style": "IPY_MODEL_d516a728cc0c4ce59bfffc0c784c77c4",
                        "value": 46827520
                    }
                },
                "d0e8c4784d8b471fbd0a1550825470d2": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_name": "HBoxModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HBoxModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HBoxView",
                        "box_style": "",
                        "children": [
                            "IPY_MODEL_b29c4de7c4cb4f6abdde19dbe4c8ae1a",
                            "IPY_MODEL_921d678d2189460aa0eef94d863927b0"
                        ],
                        "layout": "IPY_MODEL_6e92ae70a84142bbb875434128664da4"
                    }
                },
                "d516a728cc0c4ce59bfffc0c784c77c4": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_name": "ProgressStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "ProgressStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "bar_color": null,
                        "description_width": ""
                    }
                },
                "f1f0b027576d4dc1a1849a37e84405a4": {
                    "model_module": "@jupyter-widgets/base",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                }
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}